{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWUsszxxiXPH",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fctMNraAwSdh",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "He4p9b4_5eRd",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "    from torch.cuda import FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    from torch import FloatTensor\n",
    "    \n",
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "except:\n",
    "    is_in_colab = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04cdSTyW5eRg",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# вывод информации о выданном с colab GPU\n",
    "if is_in_colab:\n",
    "    !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "    !pip install gputil\n",
    "    !pip install psutil\n",
    "    !pip install humanize\n",
    "    import psutil\n",
    "    import humanize\n",
    "    import os\n",
    "    import GPUtil as GPU\n",
    "    GPUs = GPU.getGPUs()\n",
    "    gpu = GPUs[0]\n",
    "    def printm():\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "\n",
    "    printm()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSRHCdJl5eRj",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if is_in_colab:\n",
    "    drive.mount('/content/drive')\n",
    "    data_folder = r'/content/drive/My Drive/Colab/IDAO_2020/'\n",
    "else:\n",
    "    data_folder = r'./data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qztGRXl25eRm",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# баш команда для создания каталога в монитрованном гугл-диске, для хранения там данных. \n",
    "# Выполните один раз после монтирования диска, чтобы не создавать папку вручную\n",
    "# ! mkdir -p '/content/drive/My Drive/Colab/IDAO_2020/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHbuGit85eRo",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(path, model, optimizer, scheduler, train_history, val_history):\n",
    "    torch.save({\n",
    "            'epoch': len(train_history),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_history': train_history,\n",
    "            'val_history': val_history\n",
    "            }, path)\n",
    "    print('successfully saved')\n",
    "    \n",
    "def load_model(path, model, optimizer, scheduler, train_history=None, val_history=None):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    if train_history:\n",
    "        train_history = checkpoint['train_history']\n",
    "        val_history = checkpoint['val_history']\n",
    "    print('successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4k2aUKI5eRs",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def add_delta_time(df, columns=None):\n",
    "    \"\"\"\n",
    "    Добавляет столбец delta_time в секундах. Возвращает DataFrame в порядке указанном columns\n",
    "    если columns нет то возвращает все столбцы\n",
    "    \"\"\"\n",
    "    dataframe = df.sort_values(by=['sat_id', 'epoch'])\n",
    "    dataframe['delta_time'] = dataframe['epoch'].diff(1)#- dataframe['epoch'].shift(1)\n",
    "    dataframe['delta_time'] = dataframe['delta_time'].dt.seconds\n",
    "    next_sat_filter = dataframe['sat_id'] != dataframe['sat_id'].shift(1)\n",
    "    dataframe.loc[next_sat_filter, ['delta_time']] = 0\n",
    "    if not columns:\n",
    "        columns=dataframe.columns\n",
    "    return dataframe[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu-0X06_5eRx",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Norm():\n",
    "    \"\"\"\n",
    "    Нормализатор. \n",
    "    Init запоминает среднее и стандартное отклонение в данных\n",
    "    \"\"\"\n",
    "    def __init__(self, df, ignore_column=None):\n",
    "        self.mean = df.mean()\n",
    "        self.std = df.std()\n",
    "        self.l2 = None\n",
    "        self._get_l2(df)\n",
    "        if ignore_column:\n",
    "            self.mean[ignore_column] = 0\n",
    "            self.std[ignore_column] = 1\n",
    "            self.l2[ignore_column] = 1\n",
    "        \n",
    "    def _get_l2(self, df):\n",
    "        self.l2 = df.pow(2, axis=0).sum(axis=0).pow(0.5, axis=0) / df.shape[0]**0.5\n",
    "        # l2_dict = {\n",
    "        #         'Vx' : 2.63748924855871,\n",
    "        #         'Vy' : 2.6003214462464426,\n",
    "        #         'Vz' : 2.113766985332456,\n",
    "        #         'x' : 25391.823604180147,\n",
    "        #         'y' : 25609.50935919694,\n",
    "        #         'z' : 20668.126741013515,\n",
    "        #         'delta_seconds' : 3586.29103822237}\n",
    "        # Test = 120 спутников\n",
    "        l2_dict = {\n",
    "                'Vx' : 2.657422,\n",
    "                'Vy' : 2.583003,\n",
    "                'Vz' : 2.167259,\n",
    "                'x' : 25123.346693,\n",
    "                'y' : 25004.192766,\n",
    "                'z' : 20555.168916,\n",
    "                'delta_seconds' : 3530.636609}\n",
    "\n",
    "        for c in self.l2.index:\n",
    "            saved_l2 = c.replace('_sim', '')\n",
    "            if saved_l2 in l2_dict.keys():\n",
    "                self.l2[c] = l2_dict[saved_l2]\n",
    "        \n",
    "    @staticmethod\n",
    "    def columns_check(columns, df_columns):\n",
    "        if not columns:\n",
    "            return df_columns\n",
    "        return columns\n",
    "        \n",
    "    def z_norm(self, df, columns=None):\n",
    "        columns = self.columns_check(columns, df.columns)\n",
    "        return (df[columns] - self.mean[columns]) / self.std[columns]\n",
    "    \n",
    "    def l2_norm(self, df, columns=None):\n",
    "        columns = self.columns_check(columns, df.columns)\n",
    "       \n",
    "        return df[columns] / self.l2[columns]\n",
    "        \n",
    "    def back_z_norm(self, df, columns=None):\n",
    "        try:\n",
    "            columns = self.columns_check(columns, df.columns)\n",
    "        except:\n",
    "            print(\"df должен быть DataFrame или columns должен быть заполнен\")\n",
    "            return None\n",
    "        if not type(df) is pd.core.frame.DataFrame:\n",
    "            df = pd.DataFrame(data=df, columns=columns)\n",
    "        return df[columns] * self.std[columns] + self.mean[columns]\n",
    "            \n",
    "    def back_l2_norm(self, df, columns=None):\n",
    "        try:\n",
    "            columns = self.columns_check(columns, df.columns)\n",
    "        except:\n",
    "            print(\"df должен быть DataFrame или columns должен быть заполнен\")\n",
    "            return None\n",
    "        if not type(df) is pd.core.frame.DataFrame:\n",
    "            df = pd.DataFrame(data=df, columns=columns)\n",
    "            \n",
    "        return df[columns] * self.l2[columns]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJrY-r3-5eR2",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def split_data(values, coeff=0.9):\n",
    "    # coeff - доля спутников для тренировки\n",
    "    split = int(np.floor(coeff * values))\n",
    "    indices = list(range(values))\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return train_indices, test_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nli8oJYa5eR4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def split_folds(indices, n_folds):\n",
    "    # делит список индексов на n_folds частей\n",
    "    avg = len(indices) / float(n_folds)\n",
    "    result = []\n",
    "    last = 0\n",
    "    for _ in range(n_folds):\n",
    "        result.append(indices[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GE1FyDjbGX3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Data_Sat(Dataset):\n",
    "    def __init__(self, data, sequence_length=20):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = data\n",
    "        self.satellite_dict = {}\n",
    "        self.split_data()\n",
    "\n",
    "    def split_data(self):\n",
    "        # разделяет данные по каждому спутнику на отдельные секвенции длиной sequence_length каждая\n",
    "        # и записывает их в словарь self.satellite_dict\n",
    "        # Если значений не хватило до sequence то дописывает нули\n",
    "\n",
    "        for ind, satellite in enumerate(self.data['sat_id'].unique()):\n",
    "            # берем данные по одному спутнику начиная со столбца delta_seconds (нулевой столбец sat_id пропускаем)\n",
    "            sat_data = self.data.query('sat_id==@satellite').loc[:, 'x_sim':]\n",
    "            sequence_count = int(math.ceil(sat_data.shape[0] / self.sequence_length))\n",
    "            samples_sat = np.zeros((sequence_count * self.sequence_length, sat_data.shape[1]))\n",
    "            samples_sat[: sat_data.shape[0]] = sat_data.values\n",
    "            self.satellite_dict[ind] = samples_sat.reshape(sequence_count, self.sequence_length, -1)\n",
    "\n",
    "    def generate_samples(self, max_sequence_count=100, last_sequence=False):\n",
    "        # генерирует отдельные наборы последовательных секвенций, аугментируя данные: \n",
    "        # разбивает данные по одному спутнику (если sequence_count больше чем max_sequence_count)\n",
    "        # на несколько отдельных последовательностей длиной max_sequence_count\n",
    "        # например спутник с размером (sequence_count=200, sequence=20,...)\n",
    "        # функция преобразует в 2 спутника размером (max_sequence_count=100, sequence=20, ...)\n",
    "        # last_sequence=False - не добавляет последнюю sequence\n",
    "        self.samples = []\n",
    "\n",
    "        for sat in self.satellite_dict.values():\n",
    "            sequence_count = sat.shape[0]\n",
    "            if not last_sequence:\n",
    "                sequence_count -= 1\n",
    "            if  sequence_count > max_sequence_count:\n",
    "                samples_count = math.ceil(sequence_count / max_sequence_count)\n",
    "                step = (sequence_count - max_sequence_count) / (samples_count - 1)\n",
    "                for sample in range(samples_count):\n",
    "                    next_step = round(step * sample)\n",
    "                    self.samples.append(self.data_casting(sat[next_step: next_step + max_sequence_count]))\n",
    "\n",
    "    @staticmethod\n",
    "    def data_casting(data):\n",
    "        # вычитает из значений симуляции начальную ошибку.\n",
    "        # начальная ошибка равна x_sym[0] - x[0] и аналогично для y, z и т.д.\n",
    "        for i in range(6):\n",
    "            data[..., i] -= data[0, 0, i] - data[0, 0, i + 7]\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def predict_to_df(self, predicts):\n",
    "        # Переводит predict в датафрейм и добавляет id\n",
    "        self.result = pd.DataFrame()\n",
    "        \n",
    "        for ind, sequense in enumerate(predicts):\n",
    "            filters = (np\n",
    "                       .abs(self.satellite_dict[ind].reshape(sequense.shape[0], -1))\n",
    "                       .sum(axis=1) != 0\n",
    "                      )\n",
    "            self.result = self.result.append(pd.DataFrame(sequense[filters]), ignore_index=True)\n",
    "        \n",
    "        assert self.result.shape[0] == self.data.shape[0]\n",
    "        self.result['id'] = self.data['id'].values\n",
    "        self.result = self.result[['id', 0, 1, 2, 3, 4, 5]]\n",
    "        self.result.columns = ['id', 'x', 'y', 'z', 'Vx', 'Vy', 'Vz']\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param index: \n",
    "        :return: one-satellite sample [max_sequence_count, sequence_length, gt + in values]\n",
    "        \"\"\"\n",
    "        return FloatTensor(self.samples[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WpGAiMCM5eSA",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def smape(satellite_predicted_values, satellite_true_values): \n",
    "    # the division, addition and subtraction are point twice \n",
    "    return torch.mean(torch.abs(satellite_predicted_values - satellite_true_values) \n",
    "        / (torch.abs(satellite_predicted_values) + torch.abs(satellite_true_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(model, data, folds, loss_function, optimizer=None, scheduler=None,\n",
    "        epochs_count=1, batch_size=1, plot_draw=False):\n",
    "    \"\"\"\n",
    "    тренировка модели с кросс-валидацией и валидацией после каждой эпохи, валидация есть по умолчанию.\n",
    "    Выводит списки fold_train_history fold_val_history.\n",
    "    \"\"\"\n",
    "    fold_train_history = []\n",
    "    fold_val_history = []\n",
    "    \n",
    "    optim_default_state = optimizer.state_dict() if optimizer else None\n",
    "    sched_default_state = scheduler.state_dict() if scheduler else None\n",
    "        \n",
    "    for j, fold in enumerate(folds):\n",
    "\n",
    "        #Возврат оптимизатора к изначальным значениям\n",
    "        if optimizer:        \n",
    "            optimizer.load_state_dict(optim_default_state)\n",
    "\n",
    "        #Scheduler на каждом фолде заново определяется\n",
    "        if scheduler:\n",
    "            scheduler.load_state_dict(sched_default_state)\n",
    "\n",
    "        #Сброс параметров модели на каждом фолде\n",
    "        for name, module in model.named_children():\n",
    "            print('resetting ', name)\n",
    "            module.reset_parameters()\n",
    "        \n",
    "        print('Fold: ', j+1, '\\n')\n",
    "        \n",
    "        val_data = data.loc[fold]\n",
    "        val_dataset = Data_Sat(val_data, sequence_length)\n",
    "        val_dataset.generate_samples(max_sequence_count=max_sequence_count,  last_sequence=False)\n",
    "\n",
    "        train_data = data.loc[[index for nfold in folds for index in nfold if nfold != fold]]\n",
    "        train_dataset = Data_Sat(train_data, sequence_length)\n",
    "        train_dataset.generate_samples(max_sequence_count=max_sequence_count, last_sequence=False)\n",
    "\n",
    "        \n",
    "\n",
    "        train_history, val_history = fit(model, loss_function, batch_size, epoch_count, optimizer, scheduler,\n",
    "                                         train_dataset, val_dataset, plot_draw)\n",
    "                   \n",
    "        fold_val_history.append(val_history[-1])\n",
    "        fold_train_history.append(train_history[-1])\n",
    "    return fold_train_history, fold_val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit(model, loss_function, batch_size=1, epochs_count=10, optimizer=None,  \n",
    "        scheduler=None, train_dataset=None, val_dataset=None, plot_draw=False):\n",
    "    \"\"\"\n",
    "    Функция тренировки\n",
    "    return: (list) значение Score для Train и Val на каждой эпохе\n",
    "    \"\"\"\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(epochs_count):\n",
    "            name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "            epoch_train_score = 0\n",
    "            epoch_val_score = 0\n",
    "            if train_dataset:\n",
    "                epoch_train_score = do_epoch(model, loss_function, train_dataset, batch_size, \n",
    "                                              optimizer, name_prefix + 'Train:')\n",
    "                train_history.append(epoch_train_score)\n",
    "\n",
    "            if val_dataset:\n",
    "                name = '  Val:'\n",
    "                if not train_dataset:\n",
    "                    name = ' Test:'\n",
    "                epoch_val_score = do_epoch(model, loss_function, val_dataset, batch_size, \n",
    "                                             optimizer=None, name=name_prefix + name)\n",
    "                val_history.append(epoch_val_score)\n",
    "                \n",
    "                scheduler.step(epoch_val_score)\n",
    "            else:\n",
    "                scheduler.step(epoch_train_score)\n",
    "\n",
    "\n",
    "\n",
    "    if plot_draw:\n",
    "            draw_plot(train_history, val_history)\n",
    "        \n",
    "    return train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZxEdUTe5eSB",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def draw_plot(train_loss_history, val_loss_history):\n",
    "    \"\"\"\n",
    "    Рисует lineplot\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(data=[train_loss_history, val_loss_history], index=['Train', 'Val']).T\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.set(style='darkgrid')\n",
    "    ax = sns.lineplot(data=data, markers = [\"o\", \"o\"], palette='bright')\n",
    "    plt.title(\"Line Plot\", fontsize = 20)\n",
    "    plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "    plt.ylabel(\"Loss\", fontsize = 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LO-9QFlZUiu-",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, sat_data):\n",
    "    \"\"\"\n",
    "    Получает на вход модель и разделенные на sequences_count, sequence_length данные. Предсказывает реальные значение по спутнику.\n",
    "    Выводит Tensor формы (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    sequences_count, sequence_length, _ = sat_data.shape\n",
    "    result = torch.zeros((sequences_count*sequence_length, 6)).to(device)\n",
    "    model.eval()\n",
    "    model.init_hidden(1)\n",
    "    for i, seq in enumerate(sat_data):\n",
    "        inputs = FloatTensor(seq[:, None, :])\n",
    "        predicted = model(inputs)\n",
    "        \n",
    "        predicted = predicted.view(sequence_length, -1).detach()\n",
    "        \n",
    "        result[i*sequence_length : (i+1)*sequence_length] = predicted\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vZCs2D1PYS3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def do_epoch(model, loss_function, data, batch_size, optimizer=None, name=None):\n",
    "    \"\"\"\n",
    "       Генерация одной эпохи\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_error_loss = 0\n",
    "   \n",
    "    max_sequence_count, sequence_length = data[0].shape[0], data[0].shape[1]\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "    batch_count = len(loader)\n",
    "   \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batch_count) as progress_bar:               \n",
    "            for i, sample in enumerate(loader):\n",
    "                # перестановка осей. Стало - [max_sequence_count, sequence_length, batch_size, values]\n",
    "                sample = sample.permute(1, 2, 0, 3)  \n",
    "                model.init_hidden(sample.shape[2])\n",
    "                for ind, sequence in enumerate(sample):\n",
    "                    X_batch, y_batch = (sequence[..., :7]).to(device), (sequence[..., 7:]).to(device)\n",
    "                    prediction = model(X_batch)\n",
    "                    loss = smape(prediction, y_batch)\n",
    "                    error_loss = smape(X_batch[..., :6] - prediction,  X_batch[..., :6] - y_batch) # используем целевую метрику в качестве Loss\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_error_loss += error_loss.item() \n",
    "                    \n",
    "                    if is_train:\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('Epoch {} - score: {:.2f}, error score {:.2f}'.format(\n",
    "                    name, 100*(1-epoch_loss/(ind+1)/(i+1)), 100*(1-epoch_error_loss/(ind+1)/(i+1)))\n",
    "                )\n",
    "            \n",
    "            epoch_loss /= (i + 1) * max_sequence_count\n",
    "            epoch_error_loss /= (i + 1) * max_sequence_count\n",
    "            score = float((1-epoch_loss) * 100)\n",
    "            error_score = float((1-epoch_error_loss) * 100)\n",
    "            progress_bar.set_description(f'Epoch {name} - score: {score:.2f}, error score: {error_score:.2f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ij9SXAAk5eSC",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=7, output_dim=6, lstm_hidden_dim=20, \n",
    "                 lstm_layers_count=1, bidirectional=False, dropout=0,\n",
    "                 previous_hid_state=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.previous_hid_state = previous_hid_state\n",
    "        self.input_dim = input_dim \n",
    "        self.lstm_layers_count = lstm_layers_count\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "            \n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, \n",
    "                            hidden_size = self.lstm_hidden_dim,\n",
    "                            num_layers = self.lstm_layers_count,\n",
    "                            bidirectional=bidirectional,\n",
    "                            bias=True,\n",
    "                            dropout=dropout\n",
    "                           )\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_dim*(1+bidirectional), output_dim, bias=True)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h = torch.zeros(self.lstm_layers_count * (2 if bidirectional else 1), \n",
    "                             batch_size, self.lstm_hidden_dim).to(device)\n",
    "        self.c = torch.zeros(self.lstm_layers_count * (2 if bidirectional else 1), \n",
    "                             batch_size, self.lstm_hidden_dim).to(device)\n",
    "                   \n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if not self.previous_hid_state:\n",
    "            self.init_hidden(inputs.shape[1])\n",
    "        lstm_out, (self.h, self.c) = self.lstm.forward(inputs, (self.h, self.c))\n",
    "        linear_out = self.linear.forward(lstm_out)\n",
    "        self.h = self.h.detach()\n",
    "        self.c = self.c.detach()\n",
    "        \n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9dwwtQNSNoi",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#data preparation\n",
    "data = pd.read_csv(data_folder + 'train.csv', parse_dates=['epoch'])\n",
    "columns = ['id', 'sat_id', 'x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'delta_time',\n",
    "           'x', 'y', 'z', 'Vx', 'Vy', 'Vz']\n",
    "data_with_dt = add_delta_time(data, columns)\n",
    "data_with_dt.set_index(keys='sat_id', drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRbHIgL3SirL",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#data normalization\n",
    "normalizer = Norm(data_with_dt, ['id', 'sat_id',])\n",
    "norm_data = normalizer.l2_norm(data_with_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAeuctV8SpuE",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#data splitting\n",
    "np.random.seed(42)\n",
    "\n",
    "train_indices, test_indices = split_data(len(data['sat_id'].unique()), 0.8)\n",
    "folds = split_folds(train_indices, 5)\n",
    "test_data = norm_data.loc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZB3vpZJw5eSE",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data settings\n",
    "sequence_length = 5\n",
    "max_sequence_count = 25\n",
    "\n",
    "# train settings\n",
    "batch_size = 10\n",
    "epoch_count = 10\n",
    "plot_draw = False\n",
    "\n",
    "# optimizer settings\n",
    "learning_rate = 5e-3\n",
    "weight_decay = 0\n",
    "\n",
    "# model settings\n",
    "lstm_hidden_dim = 10\n",
    "lstm_hidden_lauers_count = 1\n",
    "bidirectional = False\n",
    "dropout = 0\n",
    "\n",
    "# scheduler settings\n",
    "factor = 0.5\n",
    "patience = 2\n",
    "threshold = 1e-2\n",
    "\n",
    "model = LSTM(lstm_hidden_dim=lstm_hidden_dim,\n",
    "             lstm_layers_count=lstm_hidden_lauers_count,\n",
    "             bidirectional=bidirectional,\n",
    "             dropout=dropout,\n",
    "            ).to(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "                        model.parameters(),\n",
    "                        lr=learning_rate, \n",
    "                        weight_decay=weight_decay\n",
    "                    )\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, \n",
    "                              patience=patience, verbose=True, threshold=threshold\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'score_80_v1.model'\n",
    "path = data_folder  + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save_model(path, model, optimizer, scheduler, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load_model(path, model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6hcM_Y55eSG",
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tr_hist, val_hist = cross_validation(model, norm_data, folds, loss_function, optimizer, scheduler, epochs_count=epoch_count,\n",
    "    batch_size=batch_size, plot_draw=plot_draw\n",
    "   )\n",
    "print('Mean_train_score: ', np.mean(tr_hist), ' Mean_val_score: ', np.mean(val_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Data_Sat(norm_data.loc[train_indices], sequence_length)\n",
    "train_dataset.generate_samples(max_sequence_count, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_hist, val_hist = fit(model, loss_function, batch_size=batch_size, epochs_count=epoch_count, optimizer=optimizer,  \n",
    "        scheduler=scheduler, train_dataset=train_dataset, val_dataset=None, plot_draw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = Data_Sat(test_data, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0K84BrVXMkW",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Predict test and compute score\n",
    "metric = 0\n",
    "test_predicts = []\n",
    "for sat in test_dataset.satellite_dict:\n",
    "    sat_data = test_dataset.satellite_dict[sat]\n",
    "    X = FloatTensor(sat_data[..., :7]).to(device)\n",
    "    y = FloatTensor(sat_data[..., 7:]).view(-1, 6).to(device)\n",
    "    predicts = predict(model, X)\n",
    "    test_predicts.append(predicts.cpu().detach().numpy())\n",
    "    metric += smape(predicts[y!=0].view(-1, 6), \n",
    "                    y[y!=0].view(-1, 6)\n",
    "                   )\n",
    "    \n",
    "metric /= len(test_dataset.satellite_dict)\n",
    "score = (1-metric)*100\n",
    "print(f'Test score: {float(score.cpu()):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_dataset.predict_to_df(test_predicts)\n",
    "test_dataset.result['sat_id'] = test_dataset.data['sat_id'].values\n",
    "test_dataset.result['sat_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sat_id = 98\n",
    "sat = test_dataset.data.query('sat_id==@sat_id')\n",
    "pred = test_dataset.result.query('sat_id==@sat_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for ind, col in enumerate(list(sat.loc[:,'x':'Vz'].columns)):\n",
    "    # sat.loc.__setitem__((slice(None), 'e'+col), sat[col+'_sim'] - sat[col])\n",
    "    sat.loc[slice(None), 'e'+col] = sat[col+'_sim'] - sat[col]\n",
    "    pred.loc[:, 'e'+col] = pred[col] - sat[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=6, figsize=(15, 15))\n",
    "for ind, col in enumerate(list(sat.loc[:,'ex':'eVz'].columns)):\n",
    "    # ground truth\n",
    "    sns.lineplot(data=sat, x=[i for i in range(sat.shape[0])], y=col, ax=ax[ind])\n",
    "    # sim\n",
    "    # sns.lineplot(data=sat, x=[i for i in range(sat.shape[0])], y=col+'_sim', ax=ax[ind])\n",
    "    # predicts\n",
    "    sns.lineplot(data=pred, x=[i for i in range(pred.shape[0])], y=col, ax=ax[ind])\n",
    "    ax[ind].grid(True, which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def score_without_norm():\n",
    "    # разнормализоввывает predict и считает Score\n",
    "    new_data = pd.read_csv(data_folder + 'train.csv', parse_dates=['epoch'])\n",
    "    new_data.set_index('sat_id', inplace=True, drop=True)\n",
    "    new_data = new_data.loc[test_indices]\n",
    "    new_data = new_data.loc[:, 'x':'Vz']\n",
    "    test_dataset.predict_to_df(test_predicts)\n",
    "    pred = (test_dataset.result * normalizer.l2[['id', 'x', 'y', 'z', 'Vx', 'Vy', 'Vz']].values).loc[:,'x' :]\n",
    "    gt_tensor = torch.Tensor(new_data.values)\n",
    "    pred_tensor = torch.Tensor(pred.values)\n",
    "    metric_without_norm = smape(pred_tensor, gt_tensor)\n",
    "    score_without_norm = (1-metric_without_norm)*100\n",
    "    print(f'Test score: {float(score_without_norm.cpu()):.2f}')\n",
    "score_without_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def submission():\n",
    "    test = pd.read_csv(data_folder + \"/test.csv\", parse_dates=['epoch'])\n",
    "    columns = ['id', 'sat_id', 'x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'delta_seconds']\n",
    "    test_with_dt = add_delta_time(test, columns)\n",
    "    normalizer = Norm(test_with_dt, ignore_column=['id', 'sat_id'])\n",
    "    norm_test = normalizer.l2_norm(test_with_dt)\n",
    "    test_dataset = Data_Sat(norm_test, 5)\n",
    "    test_predicts = []\n",
    "    for sat in test_dataset.satellite_dict:\n",
    "        sat_data = test_dataset.satellite_dict[sat]\n",
    "        X = FloatTensor(sat_data).to(device)\n",
    "        predicts = predict(model, X)\n",
    "        test_predicts.append(predicts.cpu().detach().numpy())\n",
    "    test_dataset.predict_to_df(test_predicts)\n",
    "    submis = test_dataset.result * normalizer.l2.drop(['sat_id','delta_seconds']).values\n",
    "    submis['id'] = submis['id'].astype('int')\n",
    "    submis.to_csv(data_folder + \"/submission_80score.csv\", index=False)\n",
    "    \n",
    "submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IDAO.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
