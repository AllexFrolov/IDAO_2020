{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWUsszxxiXPH",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import scipy.stats as st\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fctMNraAwSdh",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    \n",
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "except:\n",
    "    is_in_colab = False\n",
    "\n",
    "if is_in_colab:\n",
    "    drive.mount('/content/drive')\n",
    "    data_folder = r'/content/drive/My Drive/Colab/IDAO_2020/'\n",
    "else:\n",
    "    data_folder = r'./data/'\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# баш команда для создания каталога в монитрованном гугл-диске, для хранения там данных. \n",
    "# Выполните один раз после монтирования диска, чтобы не создавать папку вручную\n",
    "# ! mkdir -p '/content/drive/My Drive/Colab/IDAO_2020/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_model(path, model, optimizer, loss_history, train_history, val_history):\n",
    "    torch.save({\n",
    "            'epoch': len(train_history),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_history[-1],\n",
    "            'loss_history': loss_history,\n",
    "            'train_history': train_history,\n",
    "            'val_history': val_history\n",
    "            }, path)\n",
    "    print('successfully saved')\n",
    "    \n",
    "def load_model(path, model, optimizer, loss_history, train_history, val_history):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    loss_history = checkpoint['loss_history']\n",
    "    train_history = checkpoint['train_history']\n",
    "    val_history = checkpoint['val_history']\n",
    "    print('successfully loaded')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRfDlcBTiid9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_folder + 'train.csv', parse_dates=['epoch']).iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2i3nl9_iVcEl",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data.set_index(keys=data['sat_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJHqM3j3wVHt",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def split_data(values, coeff=0.7):\n",
    "    # coeff - доля трейна, остальное делится на валидацию и тест поровну\n",
    "    split = int(np.floor(coeff * values))\n",
    "    split2 = int(np.floor(values*(1-coeff)/2))\n",
    "    indices = list(range(values))\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices, test_indices = indices[:split], indices[split:split+split2], indices[split+split2:]\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "train_indices, val_indices, test_indices = split_data(len(data['sat_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yHF-S2wQHAO",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_data = data.loc[train_indices]\n",
    "test_data = data.loc[test_indices]\n",
    "val_data = data.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GE1FyDjbGX3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Data_Sat(Dataset):\n",
    "    def __init__(self, data, sequence_length=20, normalizer=None):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = data\n",
    "        self.satellite_dict = {}\n",
    "        self.normalizer = normalizer\n",
    "        self.split_data()\n",
    "\n",
    "    def split_data(self):\n",
    "        # разделяет данные по каждому спутнику на отдельные секвенции длиной sequence_length каждая\n",
    "        # и записывает их в словарь self.satellite_dict\n",
    "        # нормализует данные\n",
    "\n",
    "        for ind, satellite in enumerate(self.data['sat_id'].unique()):\n",
    "            sat_data = self.data.query('sat_id==@satellite').iloc[:, 1:]\n",
    "            sequence_count = np.floor(sat_data.shape[0] / self.sequence_length).astype('int')\n",
    "\n",
    "            samples_sat = np.zeros((sequence_count * self.sequence_length, 12))\n",
    "            samples_sat = sat_data.iloc[: sequence_count * self.sequence_length].values\n",
    "            if self.normalizer:\n",
    "                samples_sat = self.normalizer.fit_transform(samples_sat)\n",
    "\n",
    "            self.satellite_dict[ind] = samples_sat.reshape(sequence_count, self.sequence_length, 12)\n",
    "\n",
    "    def generate_samples(self, max_sequence_count=10):\n",
    "        # генерирует отдельные наборы последовательных секвенций, аугментируя данные: \n",
    "        # разбивает данные по одному спутнику (если их больше, чем max_sequence_count)\n",
    "        # на несколько отдельных последовательностей \n",
    "        # для использования их при тренировке, как разных спутников.\n",
    "        self.samples = []\n",
    "\n",
    "        for sat in self.satellite_dict.values():\n",
    "            if sat.shape[0] > max_sequence_count:\n",
    "                sequence_count = sat.shape[0]\n",
    "                samples_count = math.ceil(sequence_count / max_sequence_count)\n",
    "                step = (sequence_count - max_sequence_count) / (samples_count - 1)\n",
    "                for sample in range(samples_count):\n",
    "                    next_step = round(step * sample)\n",
    "                    self.samples.append(self.data_casting(sat[next_step: next_step + max_sequence_count]))\n",
    "\n",
    "    @staticmethod\n",
    "    def data_casting(data):\n",
    "        # вычитает из значений симуляции начальную ошибку.\n",
    "        # начальная ошибка равна x_sym[0] - x[0] и аналогично для y, z и т.д.\n",
    "        for i in range(6):\n",
    "            data[..., i + 6] -= data[0, 0, i + 6] - data[0, 0, i]\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param index: \n",
    "        :return: one-satellite sample [max_sequence_count, sequence_length, gt + in values]\n",
    "        \"\"\"\n",
    "        return torch.Tensor(self.samples[index]).type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, loss_function, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    epoch_SGP4_loss = 0\n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "    max_sequence_count = data[0].shape[1]\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        # with tqdm(total=1) as progress_bar:\n",
    "            for i, sample in enumerate(loader):\n",
    "                sample = sample.permute(1, 2, 0, 3)  # [max_sequence_count, sequence_length,  batch, gt + in values]\n",
    "                for sequence in sample:\n",
    "                    X_batch, y_batch = (sequence[...,6:]).to(device), (sequence[...,:6]).to(device)\n",
    "                    prediction = model(X_batch)\n",
    "                    loss = loss_function(prediction, y_batch)\n",
    "                    SGP4_loss = loss_function(X_batch, y_batch)\n",
    "                    \n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_SGP4_loss += SGP4_loss.item()\n",
    "\n",
    "                    if is_train:\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "\n",
    "                # progress_bar.update()\n",
    "                # progress_bar.set_description('{:>5s} Loss = {:.5f}'.format(\n",
    "                #     name, loss.item())\n",
    "                # )\n",
    "            \n",
    "            epoch_loss /= i * max_sequence_count\n",
    "            epoch_SGP4_loss /= i * max_sequence_count\n",
    "            \n",
    "            print(f'Epoch {name} - loss: {epoch_loss:.5f} , SGP4 loss: {epoch_SGP4_loss:.5f}')\n",
    "            # progress_bar.set_description('{:>5s} Loss = {:.5f}'.format(\n",
    "            #     name, epoch_loss)\n",
    "            # )\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def fit(model, loss_function, optimizer, train_data, epochs_count=1, batch_size=1,\n",
    "        val_data=None, val_batch_size=None):\n",
    "    \"\"\"\n",
    "    тренировко модели с валидацией после каждой эпохи, если валидация задана\n",
    "    \"\"\"\n",
    "        \n",
    "    if val_data and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, loss_function, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "\n",
    "        \n",
    "        if val_data:\n",
    "            val_loss = do_epoch(model, loss_function, val_data, val_batch_size, \n",
    "                                         optimizer=None, name=name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=6, output_dim=6, lstm_hidden_dim=28, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = 6, \n",
    "                            hidden_size = lstm_hidden_dim,\n",
    "                            num_layers = lstm_layers_count,\n",
    "                            bias=True)\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_dim, 6, bias=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm.forward(inputs)\n",
    "        linear_out = self.linear.forward(lstm_out)\n",
    "        return linear_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sequence_duration = 20\n",
    "train_dataset = Data_Sat(train_data, sequence_duration, Normalizer())\n",
    "train_dataset.generate_samples(max_sequence_count=100)\n",
    "val_dataset = Data_Sat(val_data, sequence_duration, Normalizer())\n",
    "val_dataset.generate_samples(max_sequence_count=100)\n",
    "print('Samples count:', len(train_dataset))\n",
    "model = LSTM().to(device)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run only train\n",
    "fit(model, loss_function, optimizer, train_dataset, epochs_count=2, batch_size=5, )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run with validation\n",
    "fit(model, loss_function, optimizer, train_dataset, epochs_count=2, batch_size=5, val_data=val_dataset)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNn/CRuVT+ZEGb/VmOyLWIL",
   "collapsed_sections": [],
   "name": "IDAO.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}