{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWUsszxxiXPH",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fctMNraAwSdh",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "    from torch.cuda import FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    from torch import FloatTensor\n",
    "    \n",
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "except:\n",
    "    is_in_colab = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# вывод информации о выданном с colab GPU\n",
    "if is_in_colab:\n",
    "    !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "    !pip install gputil\n",
    "    !pip install psutil\n",
    "    !pip install humanize\n",
    "    import psutil\n",
    "    import humanize\n",
    "    import os\n",
    "    import GPUtil as GPU\n",
    "    GPUs = GPU.getGPUs()\n",
    "    gpu = GPUs[0]\n",
    "    def printm():\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "\n",
    "    printm()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if is_in_colab:\n",
    "    drive.mount('/content/drive')\n",
    "    data_folder = r'/content/drive/My Drive/Colab/IDAO_2020/'\n",
    "else:\n",
    "    data_folder = r'./data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# баш команда для создания каталога в монитрованном гугл-диске, для хранения там данных. \n",
    "# Выполните один раз после монтирования диска, чтобы не создавать папку вручную\n",
    "# ! mkdir -p '/content/drive/My Drive/Colab/IDAO_2020/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(path, model, optimizer, loss_history, train_history, val_history):\n",
    "    torch.save({\n",
    "            'epoch': len(train_history),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_history[-1],\n",
    "            'loss_history': loss_history,\n",
    "            'train_history': train_history,\n",
    "            'val_history': val_history\n",
    "            }, path)\n",
    "    print('successfully saved')\n",
    "    \n",
    "def load_model(path, model, optimizer, loss_history, train_history, val_history):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    loss_history = checkpoint['loss_history']\n",
    "    train_history = checkpoint['train_history']\n",
    "    val_history = checkpoint['val_history']\n",
    "    print('successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRfDlcBTiid9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_folder + 'train.csv', parse_dates=['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def add_delta_time(df, columns=None):\n",
    "    \"\"\"\n",
    "    Добавляет столбец delta_time в секундах. Возвращает DataFrame в порядке указанном columns\n",
    "    если columns нет то возвращает все столбцы\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    df.sort_values(by=['sat_id', 'epoch'], inplace=True) \n",
    "    \n",
    "    df['delta_time'] = df.iloc[1:,1] - df.iloc[0:-1,1].values    \n",
    "    delta_time = df['delta_time']\n",
    "    df['delta_seconds'] = delta_time.dt.seconds\n",
    "    filters = data.iloc[:, 2] != np.insert(df.iloc[0:-1, 2].values, 0, -1)\n",
    "    df.loc[filters, ['delta_time', 'delta_seconds']] = 0\n",
    "    if not columns:\n",
    "        columns=df.columns\n",
    "    return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "columns = ['sat_id', 'delta_seconds', 'x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim',\n",
    "           'x', 'y', 'z', 'Vx', 'Vy', 'Vz']\n",
    "data_with_dt = add_delta_time(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2i3nl9_iVcEl",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data_with_dt.set_index(keys='sat_id', drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Norm():\n",
    "    \"\"\"\n",
    "    Нормализатор. \n",
    "    Init запоминает среднее и стандартное отклонение в данных\n",
    "    \"\"\"\n",
    "    def __init__(self, df, ignore_column=None):\n",
    "        self.mean = df.mean()\n",
    "        self.std = df.std()\n",
    "        self.l2 = df.pow(2, axis=0).sum(axis=0).pow(0.5, axis=0)\n",
    "        if ignore_column:\n",
    "            self.mean[ignore_column] = 0\n",
    "            self.std[ignore_column] = 1\n",
    "            self.l2[ignore_column] = 1\n",
    "    @staticmethod\n",
    "    def columns_check(columns, df_columns):\n",
    "        if not columns:\n",
    "            return df_columns\n",
    "        return columns\n",
    "        \n",
    "    def z_norm(self, df, columns=None):\n",
    "        columns = columns_check(columns, df.columns)\n",
    "        return (df[columns] - self.mean[columns]) / self.std[columns]\n",
    "    \n",
    "    def l2_norm(self, df, columns=None):\n",
    "        columns = self.columns_check(columns, df.columns)\n",
    "        return df[columns] / self.l2[columns]\n",
    "        \n",
    "    def back_z_norm(self, df, columns=None):\n",
    "        try:\n",
    "            columns = columns_check(columns, df.columns)\n",
    "        except:\n",
    "            print(\"df должен быть DataFrame или columns должен быть заполнен\")\n",
    "            return None\n",
    "        if not type(df) is pd.core.frame.DataFrame:\n",
    "            df = pd.DataFrame(data=df, columns=columns)\n",
    "            \n",
    "    def back_l2_norm(self, df, columns=None):\n",
    "        try:\n",
    "            columns = columns_check(columns, df.columns)\n",
    "        except:\n",
    "            print(\"df должен быть DataFrame или columns должен быть заполнен\")\n",
    "            return None\n",
    "        if not type(df) is pd.core.frame.DataFrame:\n",
    "            df = pd.DataFrame(data=df, columns=columns)\n",
    "            \n",
    "        return (df[columns] * self.l2[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalizer = Norm(data_with_dt, ['sat_id'])\n",
    "norm_data = normalizer.l2_norm(data_with_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJHqM3j3wVHt",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def split_data(values, coeff=0.7):\n",
    "    # coeff - доля трейна, остальное делится на валидацию и тест поровну\n",
    "    split = int(np.floor(coeff * values))\n",
    "    split2 = int(np.floor(values*(1-coeff)/2))\n",
    "    indices = list(range(values))\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices, test_indices = indices[:split], indices[split:split+split2], indices[split+split2:]\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "train_indices, val_indices, test_indices = split_data(len(data['sat_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yHF-S2wQHAO",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_data = norm_data.loc[train_indices]\n",
    "test_data = norm_data.loc[test_indices]\n",
    "val_data = norm_data.loc[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GE1FyDjbGX3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Data_Sat(Dataset):\n",
    "    def __init__(self, data, sequence_length=20):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = data\n",
    "        self.satellite_dict = {}\n",
    "        self.split_data()\n",
    "\n",
    "    def split_data(self):\n",
    "        # разделяет данные по каждому спутнику на отдельные секвенции длиной sequence_length каждая\n",
    "        # и записывает их в словарь self.satellite_dict\n",
    "\n",
    "        for ind, satellite in enumerate(self.data['sat_id'].unique()):\n",
    "            sat_data = self.data.query('sat_id==@satellite').iloc[:, 1:]\n",
    "            sequence_count = np.ceil(sat_data.shape[0] / self.sequence_length).astype('int')\n",
    "\n",
    "            samples_sat = np.zeros((sequence_count * self.sequence_length, sat_data.shape[1]))\n",
    "            samples_sat[: sat_data.shape[0]] = sat_data.values\n",
    "\n",
    "            self.satellite_dict[ind] = samples_sat.reshape(sequence_count, self.sequence_length, -1)\n",
    "\n",
    "    def generate_samples(self, max_sequence_count=10, last_sequence=False):\n",
    "        # генерирует отдельные наборы последовательных секвенций, аугментируя данные: \n",
    "        # разбивает данные по одному спутнику (если их больше, чем max_sequence_count)\n",
    "        # на несколько отдельных последовательностей \n",
    "        # для использования их при тренировке, как разных спутников.\n",
    "        self.samples = []\n",
    "        \n",
    "\n",
    "        for sat in self.satellite_dict.values():\n",
    "            sequence_count = sat.shape[0]\n",
    "            if not last_sequence:\n",
    "                sequence_count -= 1\n",
    "            if  sequence_count > max_sequence_count:\n",
    "                samples_count = math.ceil(sequence_count / max_sequence_count)\n",
    "                step = (sequence_count - max_sequence_count) / (samples_count - 1)\n",
    "                for sample in range(samples_count):\n",
    "                    next_step = round(step * sample)\n",
    "                    self.samples.append(self.data_casting(sat[next_step: next_step + max_sequence_count]))\n",
    "\n",
    "    @staticmethod\n",
    "    def data_casting(data):\n",
    "        # вычитает из значений симуляции начальную ошибку.\n",
    "        # начальная ошибка равна x_sym[0] - x[0] и аналогично для y, z и т.д.\n",
    "        for i in range(1, 7, 1):\n",
    "            data[..., i + 6] -= data[0, 0, i + 6] - data[0, 0, i]\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param index: \n",
    "        :return: one-satellite sample [max_sequence_count, sequence_length, gt + in values]\n",
    "        \"\"\"\n",
    "        return FloatTensor(self.samples[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, loss_function, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    epoch_SGP4_loss = 0\n",
    "    epoch_smape = 0\n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "    max_sequence_count, sequence_length = data[0].shape[0], data[0].shape[1]\n",
    "    batch_count = len(loader)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batch_count) as progress_bar:\n",
    "            for i, sample in enumerate(loader):\n",
    "                sample = sample.permute(1, 2, 0, 3)  # [max_sequence_count, sequence_length,  batch, gt + in values]\n",
    "                h, c = model.init_hidden(sample.shape[2])\n",
    "                for sequence in sample:\n",
    "                    X_batch, y_batch = (sequence[...,:7]).to(device), (sequence[...,7:]).to(device)\n",
    "                    \n",
    "                    prediction, (h_1, c_1) = model(X_batch, h, c)\n",
    "                    \n",
    "                    loss = loss_function(prediction, y_batch)\n",
    "                    SGP4_loss = loss_function(X_batch[...,1:], y_batch)\n",
    "                    epoch_smape += smape(prediction.detach(),\n",
    "                                         y_batch.detach())\n",
    "                    \n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_SGP4_loss += SGP4_loss.item()\n",
    "\n",
    "                    if is_train:\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                \n",
    "                    h, c = h_1.detach(), c_1.detach()\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}'.format(\n",
    "                    name, loss.item())\n",
    "                )\n",
    "            \n",
    "            epoch_loss /= (i + 1) * max_sequence_count\n",
    "            epoch_SGP4_loss /= (i + 1) * max_sequence_count\n",
    "            epoch_smape /= (i + 1) * max_sequence_count\n",
    "            score = (1-epoch_smape) * 100\n",
    "\n",
    "            loss_comparison = epoch_loss / epoch_SGP4_loss\n",
    "            \n",
    "            progress_bar.set_description(f'Epoch {name} - loss compar: {loss_comparison:.2f}, '\n",
    "                                         f'score: {score:.2f}, loss: {epoch_loss:.5f}')\n",
    "\n",
    "    return loss_comparison\n",
    "\n",
    "\n",
    "def fit(model, loss_function, optimizer=None, train_data=None, epochs_count=1, batch_size=1,\n",
    "        val_data=None, val_batch_size=None, plot_draw=False):\n",
    "    \"\"\"\n",
    "    тренировко модели с валидацией после каждой эпохи, если валидация задана\n",
    "    \"\"\"\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "        \n",
    "    if val_data and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        if train_data:\n",
    "            train_history.append(do_epoch(model, loss_function, train_data, batch_size, \n",
    "                                          optimizer, name_prefix + 'Train:')\n",
    "                                )\n",
    "\n",
    "        if val_data:\n",
    "            name = '  Val:'\n",
    "            if not train_data:\n",
    "                name = ' Test:'\n",
    "            val_history.append(do_epoch(model, loss_function, val_data, val_batch_size, \n",
    "                                         optimizer=None, name=name_prefix + name)\n",
    "                             )\n",
    "    if plot_draw:\n",
    "        draw_plot(train_history, val_history)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def smape(satellite_predicted_values, satellite_true_values): \n",
    "    # the division, addition and subtraction are pointwise \n",
    "    return torch.mean(torch.abs(satellite_predicted_values - satellite_true_values) \n",
    "        / (torch.abs(satellite_predicted_values) + torch.abs(satellite_true_values)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def draw_plot(train_loss_history, val_loss_history):\n",
    "    \"\"\"\n",
    "    Рисует lineplot\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(data=[train_loss_history, val_loss_history], index=['Train', 'Val']).T\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.set(style='darkgrid')\n",
    "    ax = sns.lineplot(data=data, markers = [\"o\", \"o\"], palette='bright')\n",
    "    plt.title(\"Line Plot\", fontsize = 20)\n",
    "    plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "    plt.ylabel(\"Loss\", fontsize = 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=7, output_dim=6, lstm_hidden_dim=20, \n",
    "                 lstm_layers_count=1, bidirectional=False, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        self.lstm_layers_count = lstm_layers_count\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "            \n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, \n",
    "                            hidden_size = self.lstm_hidden_dim,\n",
    "                            num_layers = self.lstm_layers_count,\n",
    "                            bidirectional=bidirectional,\n",
    "                            bias=True,\n",
    "                            dropout=dropout\n",
    "                           )\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_dim, output_dim, bias=True)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "            return (torch.zeros(self.lstm_layers_count * (2 if bidirectional else 1), \n",
    "                                batch_size, self.lstm_hidden_dim).to(device),\n",
    "                    torch.zeros(self.lstm_layers_count * (2 if bidirectional else 1), \n",
    "                                batch_size, self.lstm_hidden_dim).to(device)\n",
    "                   )\n",
    "\n",
    "        \n",
    "    def forward(self, inputs, h, c):\n",
    "        \n",
    "        lstm_out, (h_1, c_1) = self.lstm.forward(inputs, (h, c))\n",
    "        linear_out = self.linear.forward(lstm_out)\n",
    "        \n",
    "        return linear_out, (h_1, c_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data settings\n",
    "sequence_length = 20\n",
    "max_sequence_count = 100\n",
    "\n",
    "# train settings\n",
    "batch_size = 5\n",
    "epoch_count = 10\n",
    "plot_draw = True\n",
    "\n",
    "# optimizer settings\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "# model settings\n",
    "lstm_hidden_dim = 128\n",
    "lstm_hidden_lauers_count = 1\n",
    "bidirectional = False\n",
    "dropout = 0\n",
    "\n",
    "train_dataset = Data_Sat(train_data, sequence_length)\n",
    "train_dataset.generate_samples(max_sequence_count=max_sequence_count, last_sequence=False)\n",
    "val_dataset = Data_Sat(val_data, sequence_length)\n",
    "val_dataset.generate_samples(max_sequence_count=max_sequence_count,  last_sequence=False)\n",
    "print('Samples count:', len(train_dataset))\n",
    "\n",
    "model = LSTM(lstm_hidden_dim=lstm_hidden_dim,\n",
    "             lstm_layers_count=lstm_hidden_lauers_count,\n",
    "             bidirectional=bidirectional,\n",
    "             dropout=dropout,\n",
    "            ).to(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "                        model.parameters(),\n",
    "                        lr=learning_rate, \n",
    "                        weight_decay=weight_decay\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run with validation\n",
    "fit(model, loss_function, optimizer, train_dataset, epochs_count=epoch_count,\n",
    "    batch_size=batch_size, val_data=val_dataset, plot_draw=plot_draw\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = Data_Sat(test_data, sequence_length)\n",
    "test_dataset.generate_samples(max_sequence_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fit(model, loss_function, val_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(model, sat_data):\n",
    "    sequences_count, sequence_length, _ = sat_data.shape\n",
    "    result = torch.zeros((sequences_count*sequence_length, 6)).to(device)\n",
    "    h, c = model.init_hidden(1)\n",
    "    for i, seq in enumerate(sat_data):\n",
    "        inputs = FloatTensor(seq[:, None, :])\n",
    "        predicted, (h_1, c_1) = model(inputs, h, c)\n",
    "        \n",
    "        h, c = h_1.detach(), c_1.detach()\n",
    "        predicted = predicted.view(sequence_length, -1).detach()\n",
    "        result[i*sequence_length : (i+1)*sequence_length] = predicted\n",
    "    return result\n",
    "\n",
    "metric = 0\n",
    "for sat in test_dataset.satellite_dict:\n",
    "    sat_data = test_dataset.satellite_dict[sat]\n",
    "    X = FloatTensor(sat_data[..., :7]).to(device)\n",
    "    y = FloatTensor(sat_data[..., 7:]).view(-1, 6).to(device)\n",
    "    predicts = predict(model, X)[y!=0].view(-1, 6)\n",
    "    metric += smape(predicts, \n",
    "                    y[y!=0].view(-1, 6)\n",
    "                   )\n",
    "metric /= len(test_dataset.satellite_dict)\n",
    "score = (1-metric)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "int(score.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNn/CRuVT+ZEGb/VmOyLWIL",
   "collapsed_sections": [],
   "name": "IDAO.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}